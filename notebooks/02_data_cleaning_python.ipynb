{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0566afee",
   "metadata": {},
   "source": [
    "### Brazilian E-Commerce Data Cleaning - Phase 1\n",
    "### Portfolio Project: Data Collection & Cleaning\n",
    "### Author: Angella Nakkungu\n",
    "### Date: September 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6633d",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates comprehensive data cleaning techniques for the Brazilian E-Commerce dataset.\n",
    "It showcases technical skills in data validation, outlier detection, and standardization.\n",
    "\n",
    "Dataset Source: Brazilian E-Commerce Public Dataset by Olist (Kaggle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c12028",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd418a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.2-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.3 MB 1.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/11.3 MB 2.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.6/11.3 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.7/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.1/11.3 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 3.9 MB/s eta 0:00:00\n",
      "Using cached numpy-2.3.2-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.2 pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874f26c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp311-cp311-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading matplotlib-3.10.6-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 2.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/8.1 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 3.5 MB/s eta 0:00:00\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.2 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 matplotlib-3.10.6 pillow-11.3.0 pyparsing-3.2.3 seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096d3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e42902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BRAZILIAN E-COMMERCE DATA CLEANING PROJECT\n",
      "Phase 1: Data Collection & Cleaning\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BRAZILIAN E-COMMERCE DATA CLEANING PROJECT\")\n",
    "print(\"Phase 1: Data Collection & Cleaning\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a67398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. DATA LOADING AND INITIAL EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "# Define file paths\n",
    "data_path = '../data/raw/'\n",
    "processed_path = '../data/processed/'\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa3b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "✅ Successfully loaded 9 datasets\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "datasets = {}\n",
    "\n",
    "try:\n",
    "    # Main datasets\n",
    "    datasets['customers'] = pd.read_csv(f'{data_path}olist_customers_dataset.csv')\n",
    "    datasets['orders'] = pd.read_csv(f'{data_path}olist_orders_dataset.csv')\n",
    "    datasets['order_items'] = pd.read_csv(f'{data_path}olist_order_items_dataset.csv')\n",
    "    datasets['products'] = pd.read_csv(f'{data_path}olist_products_dataset.csv')\n",
    "    datasets['sellers'] = pd.read_csv(f'{data_path}olist_sellers_dataset.csv')\n",
    "    datasets['payments'] = pd.read_csv(f'{data_path}olist_order_payments_dataset.csv')\n",
    "    datasets['reviews'] = pd.read_csv(f'{data_path}olist_order_reviews_dataset.csv')\n",
    "    datasets['geolocation'] = pd.read_csv(f'{data_path}olist_geolocation_dataset.csv')\n",
    "    \n",
    "    # Translation datasets\n",
    "    datasets['product_category'] = pd.read_csv(f'{data_path}product_category_name_translation.csv')\n",
    "    \n",
    "    print(f\"✅ Successfully loaded {len(datasets)} datasets\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error loading datasets: {e}\")\n",
    "    print(\"Please ensure all CSV files are in the '../data/raw/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5208ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATASET OVERVIEW AND STRUCTURE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_dataset_structure(datasets_dict):\n",
    "    \"\"\"Analyze the structure and basic info of all datasets\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET STRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    structure_info = {}\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        print(f\"\\n📊 {name.upper()} DATASET:\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Store structure info\n",
    "        structure_info[name] = {\n",
    "            'shape': df.shape,\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'memory_usage': df.memory_usage(deep=True).sum()\n",
    "        }\n",
    "    \n",
    "    return structure_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d964cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET STRUCTURE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "📊 CUSTOMERS DATASET:\n",
      "   Shape: (99441, 5)\n",
      "   Columns: ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
      "\n",
      "📊 ORDERS DATASET:\n",
      "   Shape: (99441, 8)\n",
      "   Columns: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
      "\n",
      "📊 ORDER_ITEMS DATASET:\n",
      "   Shape: (112650, 7)\n",
      "   Columns: ['order_id', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value']\n",
      "\n",
      "📊 PRODUCTS DATASET:\n",
      "   Shape: (32951, 9)\n",
      "   Columns: ['product_id', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
      "\n",
      "📊 SELLERS DATASET:\n",
      "   Shape: (3095, 4)\n",
      "   Columns: ['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']\n",
      "\n",
      "📊 PAYMENTS DATASET:\n",
      "   Shape: (103886, 5)\n",
      "   Columns: ['order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']\n",
      "\n",
      "📊 REVIEWS DATASET:\n",
      "   Shape: (99224, 7)\n",
      "   Columns: ['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp']\n",
      "\n",
      "📊 GEOLOCATION DATASET:\n",
      "   Shape: (1000163, 5)\n",
      "   Columns: ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state']\n",
      "\n",
      "📊 PRODUCT_CATEGORY DATASET:\n",
      "   Shape: (71, 2)\n",
      "   Columns: ['product_category_name', 'product_category_name_english']\n"
     ]
    }
   ],
   "source": [
    "# Analyze structure\n",
    "structure_info = analyze_dataset_structure(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9c003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT - BEFORE CLEANING\n",
      "============================================================\n",
      "\n",
      "🔍 CUSTOMERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,441 rows × 5 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 ORDERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,441 rows × 8 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - order_approved_at: 160 (0.16%)\n",
      "      - order_delivered_carrier_date: 1,783 (1.79%)\n",
      "      - order_delivered_customer_date: 2,965 (2.98%)\n",
      "\n",
      "🔍 ORDER_ITEMS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 112,650 rows × 7 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 PRODUCTS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 32,951 rows × 9 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - product_category_name: 610 (1.85%)\n",
      "      - product_name_lenght: 610 (1.85%)\n",
      "      - product_description_lenght: 610 (1.85%)\n",
      "      - product_photos_qty: 610 (1.85%)\n",
      "      - product_weight_g: 2 (0.01%)\n",
      "      - product_length_cm: 2 (0.01%)\n",
      "      - product_height_cm: 2 (0.01%)\n",
      "      - product_width_cm: 2 (0.01%)\n",
      "\n",
      "🔍 SELLERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 3,095 rows × 4 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 PAYMENTS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 103,886 rows × 5 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 REVIEWS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,224 rows × 7 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - review_comment_title: 87,656 (88.34%)\n",
      "      - review_comment_message: 58,247 (58.7%)\n",
      "\n",
      "🔍 GEOLOCATION QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 1,000,163 rows × 5 columns\n",
      "   🔄 Duplicates: 261,831 (26.18%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 PRODUCT_CATEGORY QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 71 rows × 2 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. DATA QUALITY ASSESSMENT (BEFORE CLEANING)\n",
    "# ============================================================================\n",
    "\n",
    "def assess_data_quality(datasets_dict, report_name=\"BEFORE\"):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA QUALITY ASSESSMENT - {report_name} CLEANING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        print(f\"\\n🔍 {name.upper()} QUALITY ASSESSMENT:\")\n",
    "        \n",
    "        # Basic info\n",
    "        total_rows = len(df)\n",
    "        total_cols = len(df.columns)\n",
    "        \n",
    "        # Missing values analysis\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_percent = (missing_values / total_rows * 100).round(2)\n",
    "        \n",
    "        # Duplicate analysis\n",
    "        duplicates = df.duplicated().sum()\n",
    "        duplicate_percent = (duplicates / total_rows * 100).round(2)\n",
    "        \n",
    "        # Data types analysis\n",
    "        dtypes_summary = df.dtypes.value_counts().to_dict()\n",
    "        \n",
    "        print(f\"   📏 Dimensions: {total_rows:,} rows × {total_cols} columns\")\n",
    "        print(f\"   🔄 Duplicates: {duplicates:,} ({duplicate_percent}%)\")\n",
    "        \n",
    "        if missing_values.sum() > 0:\n",
    "            print(f\"   ❌ Missing values found:\")\n",
    "            for col, missing in missing_values[missing_values > 0].items():\n",
    "                print(f\"      - {col}: {missing:,} ({missing_percent[col]}%)\")\n",
    "        else:\n",
    "            print(f\"   ✅ No missing values\")\n",
    "        \n",
    "        # Store in quality report\n",
    "        quality_report[name] = {\n",
    "            'total_rows': total_rows,\n",
    "            'total_cols': total_cols,\n",
    "            'missing_values': missing_values.to_dict(),\n",
    "            'missing_percent': missing_percent.to_dict(),\n",
    "            'duplicates': duplicates,\n",
    "            'duplicate_percent': duplicate_percent,\n",
    "            'dtypes': dtypes_summary\n",
    "        }\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Assess data quality before cleaning\n",
    "quality_before = assess_data_quality(datasets, \"BEFORE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0badcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED ISSUE IDENTIFICATION\n",
      "============================================================\n",
      "\n",
      "🛒 CUSTOMERS DATASET ISSUES:\n",
      "   ❌ Invalid zip codes: 23995 records\n",
      "\n",
      "📦 ORDERS DATASET ISSUES:\n",
      "   📊 Order status distribution:\n",
      "      - delivered: 96,478\n",
      "      - shipped: 1,107\n",
      "      - canceled: 625\n",
      "      - unavailable: 609\n",
      "      - invoiced: 314\n",
      "      - processing: 301\n",
      "      - created: 5\n",
      "      - approved: 2\n",
      "   ✅ No major issues found\n",
      "\n",
      "🌍 GEOLOCATION DATASET ISSUES:\n",
      "   ❌ Duplicate records: 261,831\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. DETAILED DATA EXPLORATION AND ISSUE IDENTIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def explore_dataset_issues(datasets_dict):\n",
    "    \"\"\"Identify specific data quality issues in each dataset\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED ISSUE IDENTIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    issues_found = {}\n",
    "    \n",
    "    # CUSTOMERS Dataset Issues\n",
    "    print(f\"\\n🛒 CUSTOMERS DATASET ISSUES:\")\n",
    "    customers_df = datasets_dict['customers']\n",
    "    customer_issues = []\n",
    "    \n",
    "    # Check for invalid zip codes\n",
    "    if 'customer_zip_code_prefix' in customers_df.columns:\n",
    "        invalid_zips = customers_df[customers_df['customer_zip_code_prefix'].astype(str).str.len() != 5]\n",
    "        if len(invalid_zips) > 0:\n",
    "            customer_issues.append(f\"Invalid zip codes: {len(invalid_zips)} records\")\n",
    "            print(f\"   ❌ Invalid zip codes: {len(invalid_zips)} records\")\n",
    "    \n",
    "    # Check for missing city/state info\n",
    "    for col in ['customer_city', 'customer_state']:\n",
    "        if col in customers_df.columns:\n",
    "            missing = customers_df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                customer_issues.append(f\"Missing {col}: {missing} records\")\n",
    "                print(f\"   ❌ Missing {col}: {missing} records\")\n",
    "    \n",
    "    if not customer_issues:\n",
    "        print(\"   ✅ No major issues found\")\n",
    "    \n",
    "    issues_found['customers'] = customer_issues\n",
    "    \n",
    "    # ORDERS Dataset Issues\n",
    "    print(f\"\\n📦 ORDERS DATASET ISSUES:\")\n",
    "    orders_df = datasets_dict['orders']\n",
    "    order_issues = []\n",
    "    \n",
    "    # Check date columns\n",
    "    date_columns = [col for col in orders_df.columns if 'date' in col.lower()]\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            # Try to convert to datetime\n",
    "            pd.to_datetime(orders_df[col])\n",
    "        except:\n",
    "            order_issues.append(f\"Invalid date format in {col}\")\n",
    "            print(f\"   ❌ Invalid date format in {col}\")\n",
    "    \n",
    "    # Check order status values\n",
    "    if 'order_status' in orders_df.columns:\n",
    "        status_values = orders_df['order_status'].value_counts()\n",
    "        print(f\"   📊 Order status distribution:\")\n",
    "        for status, count in status_values.items():\n",
    "            print(f\"      - {status}: {count:,}\")\n",
    "    \n",
    "    if not order_issues:\n",
    "        print(\"   ✅ No major issues found\")\n",
    "    \n",
    "    issues_found['orders'] = order_issues\n",
    "    \n",
    "    # GEOLOCATION Dataset Issues (typically has many issues)\n",
    "    print(f\"\\n🌍 GEOLOCATION DATASET ISSUES:\")\n",
    "    geo_df = datasets_dict['geolocation']\n",
    "    geo_issues = []\n",
    "    \n",
    "    # Check for invalid coordinates\n",
    "    if 'geolocation_lat' in geo_df.columns and 'geolocation_lng' in geo_df.columns:\n",
    "        invalid_lat = geo_df[(geo_df['geolocation_lat'] < -90) | (geo_df['geolocation_lat'] > 90)]\n",
    "        invalid_lng = geo_df[(geo_df['geolocation_lng'] < -180) | (geo_df['geolocation_lng'] > 180)]\n",
    "        \n",
    "        if len(invalid_lat) > 0:\n",
    "            geo_issues.append(f\"Invalid latitude values: {len(invalid_lat)} records\")\n",
    "            print(f\"   ❌ Invalid latitude values: {len(invalid_lat)} records\")\n",
    "        \n",
    "        if len(invalid_lng) > 0:\n",
    "            geo_issues.append(f\"Invalid longitude values: {len(invalid_lng)} records\")\n",
    "            print(f\"   ❌ Invalid longitude values: {len(invalid_lng)} records\")\n",
    "    \n",
    "    # Check for duplicates (common in geolocation data)\n",
    "    duplicates = geo_df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        geo_issues.append(f\"Duplicate records: {duplicates}\")\n",
    "        print(f\"   ❌ Duplicate records: {duplicates:,}\")\n",
    "    \n",
    "    if not geo_issues:\n",
    "        print(\"   ✅ No major issues found\")\n",
    "    \n",
    "    issues_found['geolocation'] = geo_issues\n",
    "    \n",
    "    return issues_found\n",
    "\n",
    "# Explore and identify issues\n",
    "issues_identified = explore_dataset_issues(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6adcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. DATA CLEANING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def clean_customers_data(df):\n",
    "    \"\"\"Clean customers dataset\"\"\"\n",
    "    print(\"🧹 Cleaning customers data...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # Standardize zip codes\n",
    "    if 'customer_zip_code_prefix' in df_clean.columns:\n",
    "        # Convert to string and pad with zeros\n",
    "        df_clean['customer_zip_code_prefix'] = df_clean['customer_zip_code_prefix'].astype(str).str.zfill(5)\n",
    "        cleaning_log.append(\"Standardized zip codes to 5 digits\")\n",
    "    \n",
    "    # Clean city names\n",
    "    if 'customer_city' in df_clean.columns:\n",
    "        df_clean['customer_city'] = df_clean['customer_city'].str.title().str.strip()\n",
    "        cleaning_log.append(\"Standardized city names to title case\")\n",
    "    \n",
    "    # Standardize state codes\n",
    "    if 'customer_state' in df_clean.columns:\n",
    "        df_clean['customer_state'] = df_clean['customer_state'].str.upper().str.strip()\n",
    "        cleaning_log.append(\"Standardized state codes to uppercase\")\n",
    "    \n",
    "    print(f\"   ✅ Applied {len(cleaning_log)} cleaning operations\")\n",
    "    for log in cleaning_log:\n",
    "        print(f\"      - {log}\")\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "def clean_orders_data(df):\n",
    "    \"\"\"Clean orders dataset\"\"\"\n",
    "    print(\"🧹 Cleaning orders data...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # Convert date columns\n",
    "    date_columns = [col for col in df_clean.columns if 'date' in col.lower() or 'timestamp' in col.lower()]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col])\n",
    "            cleaning_log.append(f\"Converted {col} to datetime\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ⚠️ Could not convert {col} to datetime: {e}\")\n",
    "    \n",
    "    # Standardize order status\n",
    "    if 'order_status' in df_clean.columns:\n",
    "        df_clean['order_status'] = df_clean['order_status'].str.lower().str.strip()\n",
    "        cleaning_log.append(\"Standardized order status to lowercase\")\n",
    "    \n",
    "    print(f\"   ✅ Applied {len(cleaning_log)} cleaning operations\")\n",
    "    for log in cleaning_log:\n",
    "        print(f\"      - {log}\")\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "def clean_geolocation_data(df):\n",
    "    \"\"\"Clean geolocation dataset\"\"\"\n",
    "    print(\"🧹 Cleaning geolocation data...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = []\n",
    "    initial_rows = len(df_clean)\n",
    "    \n",
    "    # Remove invalid coordinates\n",
    "    if 'geolocation_lat' in df_clean.columns and 'geolocation_lng' in df_clean.columns:\n",
    "        # Remove invalid latitudes\n",
    "        df_clean = df_clean[(df_clean['geolocation_lat'] >= -90) & (df_clean['geolocation_lat'] <= 90)]\n",
    "        # Remove invalid longitudes  \n",
    "        df_clean = df_clean[(df_clean['geolocation_lng'] >= -180) & (df_clean['geolocation_lng'] <= 180)]\n",
    "        \n",
    "        removed_coords = initial_rows - len(df_clean)\n",
    "        if removed_coords > 0:\n",
    "            cleaning_log.append(f\"Removed {removed_coords} records with invalid coordinates\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    before_dedup = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = before_dedup - len(df_clean)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        cleaning_log.append(f\"Removed {duplicates_removed} duplicate records\")\n",
    "    \n",
    "    # Standardize zip codes\n",
    "    if 'geolocation_zip_code_prefix' in df_clean.columns:\n",
    "        df_clean['geolocation_zip_code_prefix'] = df_clean['geolocation_zip_code_prefix'].astype(str).str.zfill(5)\n",
    "        cleaning_log.append(\"Standardized zip codes to 5 digits\")\n",
    "    \n",
    "    # Clean city names\n",
    "    if 'geolocation_city' in df_clean.columns:\n",
    "        df_clean['geolocation_city'] = df_clean['geolocation_city'].str.title().str.strip()\n",
    "        cleaning_log.append(\"Standardized city names to title case\")\n",
    "    \n",
    "    print(f\"   ✅ Applied {len(cleaning_log)} cleaning operations\")\n",
    "    for log in cleaning_log:\n",
    "        print(f\"      - {log}\")\n",
    "    \n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "def clean_payments_data(df):\n",
    "    \"\"\"Clean payments dataset\"\"\"\n",
    "    print(\"🧹 Cleaning payments data...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # Standardize payment type\n",
    "    if 'payment_type' in df_clean.columns:\n",
    "        df_clean['payment_type'] = df_clean['payment_type'].str.lower().str.strip()\n",
    "        cleaning_log.append(\"Standardized payment types to lowercase\")\n",
    "    \n",
    "    # Ensure payment values are positive\n",
    "    if 'payment_value' in df_clean.columns:\n",
    "        negative_payments = (df_clean['payment_value'] < 0).sum()\n",
    "        if negative_payments > 0:\n",
    "            df_clean = df_clean[df_clean['payment_value'] >= 0]\n",
    "            cleaning_log.append(f\"Removed {negative_payments} records with negative payment values\")\n",
    "    \n",
    "    # Ensure installments are positive integers\n",
    "    if 'payment_installments' in df_clean.columns:\n",
    "        df_clean['payment_installments'] = df_clean['payment_installments'].fillna(1).astype(int)\n",
    "        cleaning_log.append(\"Filled missing installments with 1 and converted to integer\")\n",
    "    \n",
    "    print(f\"   ✅ Applied {len(cleaning_log)} cleaning operations\")\n",
    "    for log in cleaning_log:\n",
    "        print(f\"      - {log}\")\n",
    "    \n",
    "    return df_clean, cleaning_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10acad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "APPLYING DATA CLEANING\n",
      "============================================================\n",
      "🧹 Cleaning customers data...\n",
      "   ✅ Applied 3 cleaning operations\n",
      "      - Standardized zip codes to 5 digits\n",
      "      - Standardized city names to title case\n",
      "      - Standardized state codes to uppercase\n",
      "🧹 Cleaning orders data...\n",
      "   ✅ Applied 5 cleaning operations\n",
      "      - Converted order_purchase_timestamp to datetime\n",
      "      - Converted order_delivered_carrier_date to datetime\n",
      "      - Converted order_delivered_customer_date to datetime\n",
      "      - Converted order_estimated_delivery_date to datetime\n",
      "      - Standardized order status to lowercase\n",
      "🧹 Cleaning geolocation data...\n",
      "   ✅ Applied 3 cleaning operations\n",
      "      - Removed 261831 duplicate records\n",
      "      - Standardized zip codes to 5 digits\n",
      "      - Standardized city names to title case\n",
      "🧹 Cleaning payments data...\n",
      "   ✅ Applied 2 cleaning operations\n",
      "      - Standardized payment types to lowercase\n",
      "      - Filled missing installments with 1 and converted to integer\n",
      "🧹 Copying order_items data (no cleaning needed)\n",
      "🧹 Copying products data (no cleaning needed)\n",
      "🧹 Copying sellers data (no cleaning needed)\n",
      "🧹 Copying reviews data (no cleaning needed)\n",
      "🧹 Copying product_category data (no cleaning needed)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. APPLY CLEANING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"APPLYING DATA CLEANING\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Apply cleaning to each dataset\n",
    "cleaned_datasets = {}\n",
    "cleaning_logs = {}\n",
    "\n",
    "# Clean customers\n",
    "if 'customers' in datasets:\n",
    "    cleaned_datasets['customers'], cleaning_logs['customers'] = clean_customers_data(datasets['customers'])\n",
    "\n",
    "# Clean orders  \n",
    "if 'orders' in datasets:\n",
    "    cleaned_datasets['orders'], cleaning_logs['orders'] = clean_orders_data(datasets['orders'])\n",
    "\n",
    "# Clean geolocation\n",
    "if 'geolocation' in datasets:\n",
    "    cleaned_datasets['geolocation'], cleaning_logs['geolocation'] = clean_geolocation_data(datasets['geolocation'])\n",
    "\n",
    "# Clean payments\n",
    "if 'payments' in datasets:\n",
    "    cleaned_datasets['payments'], cleaning_logs['payments'] = clean_payments_data(datasets['payments'])\n",
    "\n",
    "# Copy other datasets (minimal cleaning needed)\n",
    "for name in ['order_items', 'products', 'sellers', 'reviews', 'product_category']:\n",
    "    if name in datasets:\n",
    "        cleaned_datasets[name] = datasets[name].copy()\n",
    "        print(f\"🧹 Copying {name} data (no cleaning needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36292acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT - AFTER CLEANING\n",
      "============================================================\n",
      "\n",
      "🔍 CUSTOMERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,441 rows × 5 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 ORDERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,441 rows × 8 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - order_approved_at: 160 (0.16%)\n",
      "      - order_delivered_carrier_date: 1,783 (1.79%)\n",
      "      - order_delivered_customer_date: 2,965 (2.98%)\n",
      "\n",
      "🔍 GEOLOCATION QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 738,332 rows × 5 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 PAYMENTS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 103,886 rows × 5 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 ORDER_ITEMS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 112,650 rows × 7 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 PRODUCTS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 32,951 rows × 9 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - product_category_name: 610 (1.85%)\n",
      "      - product_name_lenght: 610 (1.85%)\n",
      "      - product_description_lenght: 610 (1.85%)\n",
      "      - product_photos_qty: 610 (1.85%)\n",
      "      - product_weight_g: 2 (0.01%)\n",
      "      - product_length_cm: 2 (0.01%)\n",
      "      - product_height_cm: 2 (0.01%)\n",
      "      - product_width_cm: 2 (0.01%)\n",
      "\n",
      "🔍 SELLERS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 3,095 rows × 4 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n",
      "\n",
      "🔍 REVIEWS QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 99,224 rows × 7 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ❌ Missing values found:\n",
      "      - review_comment_title: 87,656 (88.34%)\n",
      "      - review_comment_message: 58,247 (58.7%)\n",
      "\n",
      "🔍 PRODUCT_CATEGORY QUALITY ASSESSMENT:\n",
      "   📏 Dimensions: 71 rows × 2 columns\n",
      "   🔄 Duplicates: 0 (0.0%)\n",
      "   ✅ No missing values\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. DATA QUALITY ASSESSMENT (AFTER CLEANING)\n",
    "# ============================================================================\n",
    "\n",
    "# Assess data quality after cleaning\n",
    "quality_after = assess_data_quality(cleaned_datasets, \"AFTER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe4bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANING IMPACT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "📈 CUSTOMERS CLEANING IMPACT:\n",
      "   📏 Rows: 99,441 → 99,441 (+0)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "   🛠️ Operations applied: 3\n",
      "\n",
      "📈 ORDERS CLEANING IMPACT:\n",
      "   📏 Rows: 99,441 → 99,441 (+0)\n",
      "   ❌ Missing values: 4,908 → 4,908 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "   🛠️ Operations applied: 5\n",
      "\n",
      "📈 ORDER_ITEMS CLEANING IMPACT:\n",
      "   📏 Rows: 112,650 → 112,650 (+0)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "\n",
      "📈 PRODUCTS CLEANING IMPACT:\n",
      "   📏 Rows: 32,951 → 32,951 (+0)\n",
      "   ❌ Missing values: 2,448 → 2,448 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "\n",
      "📈 SELLERS CLEANING IMPACT:\n",
      "   📏 Rows: 3,095 → 3,095 (+0)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "\n",
      "📈 PAYMENTS CLEANING IMPACT:\n",
      "   📏 Rows: 103,886 → 103,886 (+0)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "   🛠️ Operations applied: 2\n",
      "\n",
      "📈 REVIEWS CLEANING IMPACT:\n",
      "   📏 Rows: 99,224 → 99,224 (+0)\n",
      "   ❌ Missing values: 145,903 → 145,903 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n",
      "\n",
      "📈 GEOLOCATION CLEANING IMPACT:\n",
      "   📏 Rows: 1,000,163 → 738,332 (-261,831)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 261,831 → 0 (-261,831)\n",
      "   🛠️ Operations applied: 3\n",
      "\n",
      "📈 PRODUCT_CATEGORY CLEANING IMPACT:\n",
      "   📏 Rows: 71 → 71 (+0)\n",
      "   ❌ Missing values: 0 → 0 (+0)\n",
      "   🔄 Duplicates: 0 → 0 (+0)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. CLEANING IMPACT ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cleaning_impact(quality_before, quality_after, cleaning_logs):\n",
    "    \"\"\"Analyze the impact of data cleaning\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CLEANING IMPACT ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    impact_summary = {}\n",
    "    \n",
    "    for dataset_name in quality_before.keys():\n",
    "        if dataset_name in quality_after:\n",
    "            print(f\"\\n📈 {dataset_name.upper()} CLEANING IMPACT:\")\n",
    "            \n",
    "            before = quality_before[dataset_name]\n",
    "            after = quality_after[dataset_name]\n",
    "            \n",
    "            # Row count changes\n",
    "            rows_before = before['total_rows']\n",
    "            rows_after = after['total_rows']\n",
    "            row_change = rows_after - rows_before\n",
    "            \n",
    "            print(f\"   📏 Rows: {rows_before:,} → {rows_after:,} ({row_change:+,})\")\n",
    "            \n",
    "            # Missing value changes\n",
    "            missing_before = sum(before['missing_values'].values())\n",
    "            missing_after = sum(after['missing_values'].values())\n",
    "            missing_change = missing_after - missing_before\n",
    "            \n",
    "            print(f\"   ❌ Missing values: {missing_before:,} → {missing_after:,} ({missing_change:+,})\")\n",
    "            \n",
    "            # Duplicate changes\n",
    "            dup_before = before['duplicates']\n",
    "            dup_after = after['duplicates']\n",
    "            dup_change = dup_after - dup_before\n",
    "            \n",
    "            print(f\"   🔄 Duplicates: {dup_before:,} → {dup_after:,} ({dup_change:+,})\")\n",
    "            \n",
    "            # Applied operations\n",
    "            if dataset_name in cleaning_logs:\n",
    "                print(f\"   🛠️ Operations applied: {len(cleaning_logs[dataset_name])}\")\n",
    "            \n",
    "            impact_summary[dataset_name] = {\n",
    "                'rows_change': row_change,\n",
    "                'missing_change': missing_change,\n",
    "                'duplicates_change': dup_change,\n",
    "                'operations_count': len(cleaning_logs.get(dataset_name, []))\n",
    "            }\n",
    "    \n",
    "    return impact_summary\n",
    "\n",
    "# Analyze cleaning impact\n",
    "impact_analysis = analyze_cleaning_impact(quality_before, quality_after, cleaning_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f18863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING CLEANED DATA\n",
      "============================================================\n",
      "💾 Saved customers: ../data/processed/cleaned_customers.csv (99,441 rows)\n",
      "💾 Saved orders: ../data/processed/cleaned_orders.csv (99,441 rows)\n",
      "💾 Saved geolocation: ../data/processed/cleaned_geolocation.csv (738,332 rows)\n",
      "💾 Saved payments: ../data/processed/cleaned_payments.csv (103,886 rows)\n",
      "💾 Saved order_items: ../data/processed/cleaned_order_items.csv (112,650 rows)\n",
      "💾 Saved products: ../data/processed/cleaned_products.csv (32,951 rows)\n",
      "💾 Saved sellers: ../data/processed/cleaned_sellers.csv (3,095 rows)\n",
      "💾 Saved reviews: ../data/processed/cleaned_reviews.csv (99,224 rows)\n",
      "💾 Saved product_category: ../data/processed/cleaned_product_category.csv (71 rows)\n",
      "\n",
      "✅ Data cleaning completed successfully!\n",
      "📊 Processed 9 datasets\n",
      "📁 Files saved to: ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. SAVE CLEANED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save each cleaned dataset\n",
    "for name, df in cleaned_datasets.items():\n",
    "    filename = f'{processed_path}cleaned_{name}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"💾 Saved {name}: {filename} ({len(df):,} rows)\")\n",
    "\n",
    "print(f\"\\n✅ Data cleaning completed successfully!\")\n",
    "print(f\"📊 Processed {len(cleaned_datasets)} datasets\")\n",
    "print(f\"📁 Files saved to: {processed_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5fa017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA CLEANING PHASE 1 COMPLETED\n",
      "============================================================\n",
      "📅 Completion time: 2025-09-07 13:00:18\n",
      "📊 Datasets processed: 9\n",
      "🎯 Ready for Phase 2: Exploratory Data Analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 11. GENERATE CLEANING SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "def generate_cleaning_report():\n",
    "    \"\"\"Generate a comprehensive cleaning report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'datasets_processed': len(cleaned_datasets),\n",
    "        'quality_before': quality_before,\n",
    "        'quality_after': quality_after,\n",
    "        'cleaning_operations': cleaning_logs,\n",
    "        'impact_analysis': impact_analysis,\n",
    "        'issues_identified': issues_identified\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate final report\n",
    "final_report = generate_cleaning_report()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA CLEANING PHASE 1 COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📅 Completion time: {final_report['timestamp']}\")\n",
    "print(f\"📊 Datasets processed: {final_report['datasets_processed']}\")\n",
    "print(f\"🎯 Ready for Phase 2: Exploratory Data Analysis\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
